{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regex\n",
    "import nltk # natural language toolkit\n",
    "import spacy # industrial strength natural language processing\n",
    "import scipy # scientifici computing\n",
    "import sklearn # machine learning in python\n",
    "import warnings # Not condoned\n",
    "import numpy as np # lib handles high dimensional data, etc\n",
    "import pandas as pd # lib handles data in datastructures and timeseries\n",
    "import seaborn as sns # lib based on matplotlib\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble # combine base estimators to improve robustness\n",
    "from collections import Counter # mod w/ specialized containers\n",
    "import matplotlib.pyplot as plt # 2D plotting library \n",
    "from nltk.corpus import gutenberg, stopwords # Collection of written text ie Gutenberg\n",
    "from sklearn.model_selection import cross_val_score # \n",
    "from sklearn.linear_model import LogisticRegression # linear combination of input vars\n",
    "from sklearn.model_selection import train_test_split # Find best prediction params\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text): # Clean text\n",
    "    text = re.sub(r'--',' ',text) # regex replace argument\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split()) # join by space & split into list\n",
    "    return text     \n",
    "persuasion = gutenberg.raw('austen-persuasion.txt') # Jane Austen - Persuasion text\n",
    "alice = gutenberg.raw('carroll-alice.txt') # Lewis Carroll - Alice in Wonderland text\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion) # Chapter heading + 1 or more digits (0-9)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice) # Chapter heading + any character @ any length\n",
    "alice = text_cleaner(alice[:int(len(alice)/5)]) # text_clean function grab top portion\n",
    "persuasion = text_cleaner(persuasion[:int(len(persuasion)/5)]) # Ditto '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')  # load spacy english set\n",
    "alice_doc = nlp(alice) # Parse Alice\n",
    "persuasion_doc = nlp(persuasion) # Parse Persuasion\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents] # Group by sentences\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents] # Group by sentences\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents) # Combine books sentences into DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n",
      "Processing row 650\n",
      "Processing row 700\n",
      "Processing row 750\n",
      "Processing row 800\n",
      "Processing row 850\n",
      "Processing row 900\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(text): \n",
    "  '''\n",
    "  Bag of words is collection of the most common words.\n",
    "  We'll create a list of the 2000 most common words.\n",
    "  Filter out punctuation and stop words then return the most common words.\n",
    "  '''\n",
    "  allwords = [token.lemma_ for token in text if not token.is_punct and not token.is_stop]\n",
    "  return [item[0] for item in Counter(allwords).most_common(2000)] \n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "  '''\n",
    "  Create dataframe w/ all words in the common words set\n",
    "  Create a word count for each common word.\n",
    "  \n",
    "  '''\n",
    "  df = pd.DataFrame(columns=common_words) # Dataframe w/ columns as common words\n",
    "  df['text_sentence'] = sentences[0] # Create column w/ sentences \n",
    "  df['text_source'] = sentences[1] # Create column to categorize each book\n",
    "  df.loc[:, common_words] = 0 # Initialize count to 0\n",
    "  \n",
    "  for i, sentence in enumerate(df['text_sentence']): #Counts of words\n",
    "    words = [token.lemma_ for token in sentence if (not token.is_punct and not token.is_stop and token.lemma_ in common_words)]\n",
    "    \n",
    "    for word in words: # iterate through words\n",
    "        df.loc[i, word] += 1 # Fill row w/ word count\n",
    "    \n",
    "    if i % 50 == 0: # Insure kernel doesn't hang\n",
    "        print(\"Processing row {}\".format(i))\n",
    "  return df\n",
    "\n",
    "alicewords = bag_of_words(alice_doc) # bag of words for alice\n",
    "persuasionwords = bag_of_words(persuasion_doc) # bag of words for persuasion\n",
    "common_words = set(alicewords + persuasionwords) # Combine bags to form unique sets.\n",
    "word_counts = bow_features(sentences, common_words) # Create dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red</th>\n",
       "      <th>continually</th>\n",
       "      <th>prosecute</th>\n",
       "      <th>guidance</th>\n",
       "      <th>personableness</th>\n",
       "      <th>lazily</th>\n",
       "      <th>numerous</th>\n",
       "      <th>undesirableness</th>\n",
       "      <th>forbearance</th>\n",
       "      <th>possess</th>\n",
       "      <th>...</th>\n",
       "      <th>fall</th>\n",
       "      <th>bottle</th>\n",
       "      <th>severely</th>\n",
       "      <th>recollection</th>\n",
       "      <th>involve</th>\n",
       "      <th>certainty</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2406 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  red continually prosecute guidance personableness lazily numerous  \\\n",
       "0   0           0         0        0              0      0        0   \n",
       "1   0           0         0        0              0      0        0   \n",
       "2   0           0         0        0              0      0        0   \n",
       "3   0           0         0        0              0      0        0   \n",
       "4   0           0         0        0              0      0        0   \n",
       "\n",
       "  undesirableness forbearance possess  ... fall bottle severely recollection  \\\n",
       "0               0           0       0  ...    0      0        0            0   \n",
       "1               0           0       0  ...    0      0        0            0   \n",
       "2               0           0       0  ...    0      0        0            0   \n",
       "3               0           0       0  ...    0      0        0            0   \n",
       "4               0           0       0  ...    0      0        0            0   \n",
       "\n",
       "  involve certainty                                      text_sentence  \\\n",
       "0       0         0  (Alice, was, beginning, to, get, very, tired, ...   \n",
       "1       0         0  (So, she, was, considering, in, her, own, mind...   \n",
       "2       0         0  (There, was, nothing, so, VERY, remarkable, in...   \n",
       "3       0         0                                      (Oh, dear, !)   \n",
       "4       0         0                                      (Oh, dear, !)   \n",
       "\n",
       "  text_source num_words num_punct  \n",
       "0     Carroll        57        10  \n",
       "1     Carroll        55         7  \n",
       "2     Carroll        28         3  \n",
       "3     Carroll         2         1  \n",
       "4     Carroll         2         1  \n",
       "\n",
       "[5 rows x 2406 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.962432915921288\n",
      "Test score: 0.8743315508021391\n",
      "Accuracy: %0.2f (+/- %0.2f) 0.8284259894760924\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier() \n",
    "Y = word_counts['text_source'] # Book title\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "scores_rfc = cross_val_score(rfc, X, Y, cv=10)\n",
    "print('Train score:', rfc.score(X_train, y_train))\n",
    "print('Test score:', rfc.score(X_test, y_test))\n",
    "print('Accuracy: %0.2f (+/- %0.2f)', scores_rfc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: %0.2f (+/- %0.2f) 0.8380004575611988\n"
     ]
    }
   ],
   "source": [
    "svm = svm.SVC(kernel='linear', C=1)\n",
    "scores_svm = cross_val_score(svm, X, Y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)', scores_svm.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9534883720930233\n",
      "Test score: 0.8796791443850267\n",
      "Accuracy: %0.2f (+/- %0.2f) 0.8691374971402425\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print('Train score:', lr.score(X_train, y_train))\n",
    "print('Test score:', lr.score(X_test, y_test))\n",
    "scores_lr = cross_val_score(lr, X, Y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)', scores_lr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8801431127012522\n",
      "Test score: 0.8288770053475936\n",
      "Accuracy: %0.2f (+/- %0.2f) 0.8112903225806452\n"
     ]
    }
   ],
   "source": [
    "ensemble = ensemble.GradientBoostingClassifier()\n",
    "train = ensemble.fit(X_train, y_train)\n",
    "print('Train score:', ensemble.score(X_train, y_train))\n",
    "print('Test score:', ensemble.score(X_test, y_test))\n",
    "scores_ensemble = cross_val_score(ensemble, X, Y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)', scores_ensemble.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n"
     ]
    }
   ],
   "source": [
    "emma = gutenberg.raw('austen-emma.txt') # Get text\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma) # Remove Volume headings\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma) # Remove Chapter headings\n",
    "emma = text_cleaner(emma[:int(len(emma)/60)]) # Get 60th of total corpus\n",
    "emma_doc = nlp(emma) # Parse corpus\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents] # group by sent\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents] # group by sent\n",
    "emma_sentences = pd.DataFrame(emma_sents) # Bag of words from emma word counts\n",
    "emma_bow = bow_features(emma_sentences, common_words) # Same common words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test score: 0.5739130434782609\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>153</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>130</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen      153       17\n",
       "Carroll     130       45"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Emma_test = np.concatenate((X_train[y_train[y_train=='Carroll'].index], # Combine X data from Alice+Emma\n",
    "                              emma_bow.drop(['text_sentence','text_source'], 1)), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'], # Combine y data from Alice+Emma\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "print('\\nTest score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by increasing the words and changing the size of the bag of words you can get the accuracy for the logistic regression to above 90%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
